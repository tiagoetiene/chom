#!/usr/bin/env node

_ = require("underscore");
MongoClient = require('mongodb').MongoClient;

args = require('./save/args');
utils = require('./libs/utils');

// Input arguments
var inputDatabase = program.database;
var inputKeys = program.keys;

var totalNumberOfTweets = {};
var database;
var queue = {};

if( !inputDatabase ) {
	process.stderr.write( "\n  * Missing database information\n" );
	program.help();
}

if( !inputKeys ) {
	process.stderr.write( "\n  * Missing keys to be used\n" );
	program.help();	
}

MongoClient.connect( inputDatabase.mongo_url, function( err, db ) {

	if( err )  {
		console.log( '* Error found while connecting to the database:', err );
		throw err;
	}
	database = db;

	var collection = database.collection( inputDatabase.collection );
	var isSaving = false;

	setInterval( function() { 

		//
		// If we are in the process of saving data to the database,
		// let's to start saving more data and overloaded the
		// server in the process
		//
		if( isSaving == true )
			return;


		//
		// Ok, we will start a potentially expensive operation
		// Setting isSaving to true will lock the network communication
		// for the current data in queue
		//
		isSaving = true;

		//
		// MongoDB bullk operation. We don't care about the order
		// in which the data is save.
		//
		var bulkOperations = collection.initializeUnorderedBulkOp();

		//
		// Counting operation. We do not want to call .execute()
		// if we do not have to
		//
		var operationCounter = 0;

		//
		// For each element in queue, let's schedule
		// an operation
		//
		_.each( queue, function( value, key ) {

			if( value.modified ) {

				var query = {};

				var datum =  JSON.parse( JSON.stringify( value ) ) ;

				_.each( inputKeys, function( key ) {

					query[ key ] = value[ key ];

					delete datum[ key ];

				} );
				delete datum[ "modified" ];

				operationCounter++;
				bulkOperations.find( query ).upsert().updateOne( { $set : datum } );
			}
		} );

		if( operationCounter == 0 ) {

			// 
			// If no operation was scheduled, let's continue
			//
			isSaving = false;

		} else {
			//
			// Marking everything as modified
			// 
			//
			// Let's try to save the data, if for some reason it doesn't succeed,
			// then print the error
			//
			try {
				var timeout = 5000000;

				bulkOperations.execute( { w: 1, wtimeout : timeout }, function( err, ret ) {

					if( err ) 
						process.stderr.write( "* error: " + JSON.stringify( err )  + "\n" );
					
					isSaving = false;
					_.each( queue, function( value, key ) { value.modified = false; } );

				} );

			} catch ( err ) {

				process.stderr.write( "* exception " + JSON.stringify( err ) + "\n" );
				console.log( err )
				
				//
				// If a error accurs, we discard whatever was in the queue 
				// up until this point, restart bulk operations, and set 
				// isRunning to false
				// 
				isSaving = false;
			}
		}
	}, 10000 );
});

function getKey( datum ) {
	var key = "";
	_.each( inputKeys, function( k, idx ) { 
		key += datum[ k ]; 
	} );
	return key;
}


utils.readJSONFromSTDIN( function( datum ) {

	if( _.isEmpty( datum ) ) {

		process.stderr.write( "* Closing db " );
		database.close();
		process.exit();
		return;

	}


	// Extracting key and value from data
	var key = getKey( datum );

	// console.log( key );
	

	if( _.has( queue, key ) ) {
		datum.modified = queue[ key ].modified;
	}	


	// If new data is different from old data, 
	// then update it
	if( _.isEqual( queue[ key ], datum ) == false ) {
		datum.modified = true;
		queue[ key ] = datum;
	}

} );

// Peridiocally outputs the number of tweets saved 
utils.printToFile( "/tmp/chom.save.txt", totalNumberOfTweets );